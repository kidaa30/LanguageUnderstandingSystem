<!doctype html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<style>
h1,
h2,
h3,
h4,
h5,
h6,
p,
blockquote {
    margin: 0;
    padding: 0;
}
body {
    font-family: "Helvetica Neue", Helvetica, "Hiragino Sans GB", Arial, sans-serif;
    font-size: 13px;
    line-height: 18px;
    color: #737373;
    background-color: white;
    margin: 10px 13px 10px 13px;
}
table {
	margin: 10px 0 15px 0;
	border-collapse: collapse;
}
td,th {	
	border: 1px solid #ddd;
	padding: 3px 10px;
}
th {
	padding: 5px 10px;	
}

a {
    color: #0069d6;
}
a:hover {
    color: #0050a3;
    text-decoration: none;
}
a img {
    border: none;
}
p {
    margin-bottom: 9px;
}
h1,
h2,
h3,
h4,
h5,
h6 {
    color: #404040;
    line-height: 36px;
}
h1 {
    margin-bottom: 18px;
    font-size: 30px;
}
h2 {
    font-size: 24px;
}
h3 {
    font-size: 18px;
}
h4 {
    font-size: 16px;
}
h5 {
    font-size: 14px;
}
h6 {
    font-size: 13px;
}
hr {
    margin: 0 0 19px;
    border: 0;
    border-bottom: 1px solid #ccc;
}
blockquote {
    padding: 13px 13px 21px 15px;
    margin-bottom: 18px;
    font-family:georgia,serif;
    font-style: italic;
}
blockquote:before {
    content:"\201C";
    font-size:40px;
    margin-left:-10px;
    font-family:georgia,serif;
    color:#eee;
}
blockquote p {
    font-size: 14px;
    font-weight: 300;
    line-height: 18px;
    margin-bottom: 0;
    font-style: italic;
}
code, pre {
    font-family: Monaco, Andale Mono, Courier New, monospace;
}
code {
    background-color: #fee9cc;
    color: rgba(0, 0, 0, 0.75);
    padding: 1px 3px;
    font-size: 12px;
    -webkit-border-radius: 3px;
    -moz-border-radius: 3px;
    border-radius: 3px;
}
pre {
    display: block;
    padding: 14px;
    margin: 0 0 18px;
    line-height: 16px;
    font-size: 11px;
    border: 1px solid #d9d9d9;
    white-space: pre-wrap;
    word-wrap: break-word;
}
pre code {
    background-color: #fff;
    color:#737373;
    font-size: 11px;
    padding: 0;
}
sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}
* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:10px auto;
    }
}
@media print {
	body,code,pre code,h1,h2,h3,h4,h5,h6 {
		color: black;
	}
	table, pre {
		page-break-inside: avoid;
	}
}
</style>
<title>First Mid-Term Project</title>
<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax:{inlineMath:[['$$$','$$$']]}});</script><script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<h1>First Mid-Term Project</h1>

<h2>Overview</h2>

<p>This is the first mid-term project for the course of Language Understanding Systems.</p>

<p>Author: <a href="https://twitter.com/youtux">Alessio Bogon</a>.</p>

<h2><span id="usage">Long story short: usage</span></h2>

<p>In order to run the program, you need to have either:</p>

<ul>
<li>in <strong>atis/custom.wl</strong> the file containing the sentences to test, or</li>
<li>in <strong>atis/custom.tagged</strong><sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> the test file with concept tags.</li>
</ul>


<p>In case you want use the first one, just type</p>

<pre><code>make from_wl
</code></pre>

<p>Otherwise if you chose the latter, just type</p>

<pre><code>make from_tagged
</code></pre>

<p>In either ways you will find in <strong>build/output.txt</strong> the output file generated by the program, and in case you chose to give the reference file as input, you'll find also in <strong>build/comparison.txt</strong> a comparison of them with the calculated CER.</p>

<p>As we will see later, some of the finite state machines are dynamically generated by a script. In order to generate the source code for these automatons you will need to call</p>

<pre><code>make parser
</code></pre>

<h2><span id="description">Description</span></h2>

<p>The main steps that have been followed in order to build the project are the following:</p>

<ul>
<li>Understand the training set to get an overview of the concepts we want to recognize</li>
<li>Write a first draw of the grammar</li>
<li>Write a python script (<strong>scripts/build_parser.py</strong>) that creates the w2c.fst (word2concepts finite state transducer) starting from concept dictionaries (see <a href="#w2c">later</a>)</li>
<li>Refine the grammar and the python script until results are satisfying</li>
</ul>


<p>In order to automate the procedure of tagging a sentence, the project uses a combination of <em>Makefile</em>, <em>python</em> scripts, <a href="http://www2.research.att.com/~fsmtools/fsm/"><em>fsm</em></a> and <a href="http://www2.research.att.com/~fsmtools/grm/"><em>grm</em></a> tools by <em>AT&amp;T</em>. This tools will help us to build three final state machines:</p>

<ul>
<li><strong>w2c.fst</strong>, the one that converts words (both known and unknown) to concepts</li>
<li><strong>slu.fsa</strong>, the acceptor generated by the grammar,</li>
<li><strong>null.fst</strong>, the transducer that convert words that are no more useful to <em>null</em></li>
</ul>


<p>In a nutshell, the <strong>Makefile</strong> will:</p>

<ul>
<li>build the <em>fsm</em>s and pack them in <strong>tagger.fst</strong></li>
<li>take the input list of utterances, convert everyone in the corresponding <em>fsa</em> and pack them into one big file using <code>farcompilestrings</code>.</li>
<li>Compose every single utterance with the <strong>tagger.fst</strong> and put the result into another <em>fararchive</em> (<em>output.far</em>).</li>
<li>Compare the reference file (if available) with the generated one, and show the results.</li>
</ul>


<h3><span id="w2c">Words to concepts conversion</span></h3>

<p>The file <strong>build/w2c.txt</strong> is the source code of the transducer that maps words to concepts (e.g. <em>dallas</em> to <em>city_name</em>, <em>fromloc.city_name</em>, <em>toloc.city_name</em>, ...). In order to create it, we use a python script, <strong>scripts/build_parser.py</strong> that uses a knowledge base of concepts: in the <strong>dictionaries/concepts/</strong> directory there are plenty of files, for instance <strong>city_name.txt</strong>. When the <strong>build_parser.py</strong> is invoked, it reads every file from this directory<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>, and if the filename matches one of the concepts, all the lines in the file will be mapped to that concept.</p>

<p>For instance, if the content of the file <strong>dictionaries/concepts/city_name.txt</strong> is</p>

<pre><code>dallas
denver
san-francisco
</code></pre>

<p>then these lines will be mapped to the right concepts<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup> in the <strong>w2c.txt</strong> source code:</p>

<pre><code>0   0   dallas  stoploc.city_name   0
0   0   dallas  fromloc.city_name   0
0   0   dallas  toloc.city_name 0
0   0   dallas  dallas  0
0   0   dallas  city_name   0
0   0   dallas  null    4
0   0   denver  stoploc.city_name   0
...
</code></pre>

<p><strong>w2c.txt</strong> is generated using also another dictionary, <strong>dictionaries/basic_words.txt</strong>. This one only has words that are meaningful to the grammar in order to take a decision, but they will be mapped to null once the <em>best path</em> has been chosen.</p>

<p>Another important thing is the handling of <em>unknown</em> words. We don't know anything about that these words and they can be anything: an airport, a city, a date, a mistake, gibberish, etc. Due to this unpredictability we map the <em>unknown</em> to all the possible concepts, including of course <em>null</em>. When doing this we use different weights and we can see them in the <strong>build_parser.py</strong> script:</p>

<pre><code>def words2concepts(...):
    # define weights
    w = {
        'unk2concepts': 4,
        'unk2null': 2,
        'basic2null': 5,
        'concepts2null': 4,
    }
    ...
</code></pre>

<p>As we can see, <em>unknown</em> words are mapped to concepts with weight 4, and to <em>null</em> with weight 2. This has been done since once we have a big-enough knowledge base it is more likely that something is useless (i.e. <em>null</em>) rather than a useful concept.</p>

<p>There are other weights declared, but these are just selected to fine-grain the accuracy of the system.</p>

<h3><span id="grammar">The Grammar</span></h3>

<p>The grammar that will take the decision for the right tag is located in <strong>automatons/slu.txt</strong>. The grammar is meant to cover a big portion of the utterances given in the development and test sets. Many concepts are not implemented to keep the grammar readable and not overkilling in terms of computation time. Keep in mind that <code>grm</code> tools allow only regular grammar, so we don't expect to have the power of a real context-free grammar.</p>

<p>The idea used to build the grammar is that we want to accept every utterance and that we want to be as much flexible as possible when tagging.</p>

<p>For example, the purpose of the lines<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup></p>

<pre><code>S                   0 &lt;eps&gt;
S                   0 GARBAGE S
...
GARBAGE             0 GARBAGE_S GARBAGE | 0 &lt;eps&gt;
GARBAGE_S           1 null | 0 what | 0 which | 0 like to | ...
</code></pre>

<p>is to accept everything in the worst case, since all the words are mapped to null by the <em>w2c</em>. Note that when reading a <em>null</em> word we increase the weight by 1, since we want to treat a word as <em>null</em> if no other paths are possible.</p>

<p>The lines</p>

<pre><code>S                   0 leaving FROMLOC for TOLOC S | 0 between FROMLOC and TOLOC S  | 0 from FROMLOC to STOPLOC to TOLOC S
S                   0 FROM_SEP FROMLOC TO_SEP TOLOC S
</code></pre>

<p>are straight forward: they will tag utterances like <code>show me the flights leaving boston for denver</code> or <code>flights between san-francisco and new-york</code> or <code>from miami to los-angeles</code>, and they are probably the most used production rules during the tagging process.</p>

<p>Then we have a more general production</p>

<pre><code>S                   0 START_STMT GARBAGE START_FRAME S
S                   0 ARRIVE_STMT GARBAGE ARRIVE_FRAME S
S                   0 STOP_STMT GARBAGE STOP_FRAME S
S                   1 RETURN_STMT GARBAGE RETURN_FRAME S
</code></pre>

<p>that will parse <em>frames</em> of the utterance that are respectively informations about <em>leaving</em>, <em>arriving</em>, <em>stop</em> and <em>return</em>. Note that we get to the <code>RETURN_FRAME</code> with cost 1, since it is less likely to have <em>return</em> informations rather than <em>arrive</em> informations.</p>

<p>We can now jump to the <code>START_FRAME</code> productions:</p>

<pre><code>START_FRAME         0 FROM_SEP FROMLOC START_FRAME_CONT
START_FRAME         0 LEAVING_TIME START_FRAME_CONT
START_FRAME         0 LEAVING_DATE START_FRAME_CONT
START_FRAME_CONT    0 START_FRAME START_FRAME_CONT | 0 &lt;eps&gt;
</code></pre>

<p>Once we get to the <code>START_FRAME</code>, we may read a <code>FROM_SEP</code> and then we'll wait for a location, or we can read something related to the time/date and go into the <code>LEAVING_TIME</code>/<code>_DATE</code>. In both ways, thanks to the <code>START_FRAME_CONT</code> rule we can either keep staying in the frame or exit from it using the <code>&lt;eps&gt;</code> shortcut. This allow us to read further informations that should be treated as <em>leaving</em> informations. The same goes for <code>ARRIVE_FRAME</code> and <code>RETURN_FRAME</code>.</p>

<p>All the other production rules are probably not worth the time to be explained, since they are pretty simple.</p>

<h2><span id="results">Results</span></h2>

<p>The output of the program can be found in two forms:</p>

<ul>
<li><strong>build/output.far</strong> in form of a <em>fararchive</em></li>
<li><strong>build/output.txt</strong> in form of two-columns plain text (just like the tagged-input one)</li>
</ul>


<p>If we want to compare the result with the reference file there is a script, <strong>compare_outputs.py</strong>, that compares tagged utterances one by one, using the CER. Note that we use the <em>strong</em> version of the CER: all the tags are considered in the evaluation, including null, and the order matters. For instance, if we have something like:</p>

<table>
<thead>
<tr>
<th>Input        </th>
<th> Reference             </th>
<th> Output</th>
</tr>
</thead>
<tbody>
<tr>
<td>flights      </td>
<td> null                  </td>
<td> null</td>
</tr>
<tr>
<td>from         </td>
<td> <strong>null</strong>              </td>
<td> <strong>fromloc.city_name</strong></td>
</tr>
<tr>
<td>dallas       </td>
<td> <strong>fromloc.city_name</strong> </td>
<td> <strong>null</strong></td>
</tr>
<tr>
<td>to           </td>
<td> null                  </td>
<td> null</td>
</tr>
<tr>
<td>san-francisco</td>
<td> toloc.city_name       </td>
<td> toloc.city_name</td>
</tr>
</tbody>
</table>


<p>the CER will be 2/5, since we have 2 unaligned concepts on rows 2 and 3.</p>

<p>The result of the comparison is stored in <strong>build/comparison.txt</strong>. At the end of the file we will see two lines like the followings:</p>

<pre><code>{'sum_n_concepts': 8333, 'sum_edit_d': 487}
CER_total=0.0584423376935
</code></pre>

<p>The <code>sum_n_concepts</code> variable stores the total number of words, while <code>sum_edit_d</code> accumulates the edit distance of all the utterances.
<code>CER_total</code> is the weighted average of all the calculated <em>CER</em>s.</p>

<p>In the following table we can find the results obtained using the development set and the test set:</p>

<table>
<thead>
<tr>
<th>File name         </th>
<th> Computation time </th>
<th> Total CER</th>
</tr>
</thead>
<tbody>
<tr>
<td>atis.hlti.100.dev </td>
<td> 1:22,53          </td>
<td> 0.00728</td>
</tr>
<tr>
<td>atis.test         </td>
<td> 12:12,69         </td>
<td> 0.05844</td>
</tr>
</tbody>
</table>


<p>Unfortunately the computation time is pretty long and this is partially due to the fact that after the <em>w2c</em> conversion every word assumes non-deterministically many concepts. A solution to this problem will be described in the next paragraph.</p>

<h2><span id="furtherwork">Further work and optimisation</span></h2>

<p>The heavy computation time is due to the fact that every input word is converted in all the possible concepts (with different weights of course) in order to get a good flexibility.</p>

<p>If we have a very good knowledge base of all the possible airports, cities, etc. we can avoid to map every word to all the concepts, and map it only to the matching ones.</p>

<p>Furthermore, another optimisation would be to pre-process the utterances by recognizing numbers like times, dates, flight numbers, etc. using a regular expression. For instance, instead of having a 1540-lines dictionary for the <code>time</code> concept, a simple regular expression would be much more efficient.</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>The tagged/reference file must agree to a two-columns style, the first containing one word of the utterance and the second containing the associated concept. Sentences must be separated with a blank new-line. See <strong>atis/atis.hlti.100.dev</strong> for reference.<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p>Actually, <strong>build_parser.py</strong> only looks for filenames that doesn&rsquo;t end with <code>.ignore</code><a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
<li id="fn:3">
<p>Every word is also mapped to itself and to null. To itself because we may want to handle special names in our grammar. For instance, <code>the</code> can be the article or can be the IATA code for the <a href="http://en.wikipedia.org/wiki/Teresina_Airport">Teresina Airport</a>. To null because we may want to ignore the word. Note that these actions have different weights.<a href="#fnref:3" rev="footnote">&#8617;</a></p></li>
<li id="fn:4">
<p>The <code>GARBAGE_S</code> non-terminal is a single part of useless words (e.g. <em>the</em>, <em>what</em>, <em>like to</em>, etc.), while <code>GARBAGE</code> is zero or more of <code>GARBAGE_S</code>.<a href="#fnref:4" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

</body>
</html>